{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Optimizers\n",
    "from keras import optimizers\n",
    "\n",
    "#Results\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Read the csvs into dataframes\n",
    "# train_csv = pd.read_csv('./train.csv')\n",
    "# test_csv = pd.read_csv('./test.csv')\n",
    "# features_csv = pd.read_csv('./features.csv')\n",
    "# stores_csv = pd.read_csv('./stores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Merge the features and store information into the train_csv\n",
    "# train_merged_df = (train_csv.merge(stores_csv, how='left', on='Store')).merge(features_csv, how='left', on = ['Date', 'Store'])\n",
    "# del train_merged_df['IsHoliday_y'] #delete duplicate column\n",
    "# train_merged_df = train_merged_df.rename(columns={\"IsHoliday_x\": \"IsHoliday\"})\n",
    "\n",
    "# train_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Fill empty markkdown sales with 0\n",
    "# train_merged_df['MarkDown1'] = train_merged_df['MarkDown1'].fillna(0)\n",
    "# train_merged_df['MarkDown2'] = train_merged_df['MarkDown2'].fillna(0)\n",
    "# train_merged_df['MarkDown3'] = train_merged_df['MarkDown3'].fillna(0)\n",
    "# train_merged_df['MarkDown4'] = train_merged_df['MarkDown4'].fillna(0)\n",
    "# train_merged_df['MarkDown5'] = train_merged_df['MarkDown5'].fillna(0)\n",
    "\n",
    "# train_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Convert date column to datetime data type\n",
    "# train_merged_df['Date'] = pd.to_datetime(train_merged_df['Date'])\n",
    "\n",
    "# #Extract year, month and day from the datestamps\n",
    "# train_merged_df['Year'] = train_merged_df['Date'].dt.year\n",
    "# train_merged_df['Month'] = train_merged_df['Date'].dt.month\n",
    "# train_merged_df['Day'] = train_merged_df['Date'].dt.day\n",
    "\n",
    "# train_merged_df.drop(['Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df.to_csv(r'.\\Merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for rows with na entries\n",
    "train_merged_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add two columns - 4-week rolling average ('rolling_4_sales') and previous week sales('t-1_sales')\n",
    "train_merged_df['rolling_4_sales'] = train_merged_df['Weekly_Sales'].rolling(4).mean()\n",
    "train_merged_df['t-1_sales'] = train_merged_df['Weekly_Sales'].shift(periods=1, freq=None, axis=0)\n",
    "train_merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_merged_df.count())\n",
    "train_merged_df = train_merged_df.dropna(how='any')\n",
    "print(train_merged_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_merged_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling categorical columns - label encoding & one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use get_dummies to one-hot encode the 'Type'  and 'IsHoliday' column - one-hot encoding this column as it has >2 unique values\n",
    "train_encoded_df = pd.get_dummies(train_merged_df, columns=[\"Type\", 'IsHoliday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_mat = train_encoded_df.corr()\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "\n",
    "sb.heatmap(C_mat, vmin =-1, vmax = 1, square=True, cmap='RdYlGn', annot=True, fmt='.1f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_encoded_df['Weekly_Sales'].values\n",
    "y_train = y_train.reshape(-1,1)\n",
    "X_train = train_encoded_df.drop('Weekly_Sales', axis=1)\n",
    "feature_names = X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale and transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "y_scaler = MinMaxScaler().fit(y_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "y_train_scaled = y_scaler.transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a normal sequential neural network\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(units=32, kernel_initializer='normal', activation='relu', input_dim=len(X_train.columns))) # hidden and input layers\n",
    "nn_model.add(Dense(units=1, kernel_initializer='normal',activation='linear')) # output layer\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001, decay = 0.005)\n",
    "\n",
    "nn_model.compile(optimizer= opt,\n",
    "                loss='mean_absolute_error',\n",
    "                metrics=['mean_absolute_error'])\n",
    "    \n",
    "nn_model_var = nn_model.fit(\n",
    "                X_train_scaled,\n",
    "                y_train_scaled,\n",
    "                epochs=50,\n",
    "                verbose=2)\n",
    "\n",
    "plt.plot(nn_model_var.history['loss'])\n",
    "plt.title('Normal Neural Network Model Loss (lr=0.001, decay = 0.005)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train Loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001, decay = 0.05, amsgrad = True)\n",
    "\n",
    "nn_model.compile(optimizer= 'adam',\n",
    "                loss='mean_absolute_error',\n",
    "                metrics=['mean_absolute_error'])\n",
    "    \n",
    "nn_model_var = nn_model.fit(\n",
    "                X_train_scaled,\n",
    "                y_train_scaled,\n",
    "                epochs=50,\n",
    "                verbose=2)\n",
    "\n",
    "plt.plot(nn_model_var.history['loss'])\n",
    "plt.title('Normal Neural Network Model Loss (lr=0.001, decay = 0.05, amsgrad = True)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train Loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_train_loss, nn_model_train_accuracy = nn_model.evaluate(\n",
    "    X_train_scaled, y_train_scaled, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network Evaluation - Train Loss: {nn_model_train_loss}, Train Accuracy: {nn_model_train_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predicted_nn = nn_model.predict(X_test_scaled)\n",
    "\n",
    "# MAE = mean_absolute_error(y_test_scaled , y_predicted_nn)\n",
    "# MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = Sequential()\n",
    "dnn_model.add(Dense(units=32, kernel_initializer='normal', activation='relu', input_dim=len(X_train.columns))) # hidden and input layers\n",
    "dnn_model.add(Dense(units=64, kernel_initializer='normal', activation='relu')) # hidden and input layers\n",
    "dnn_model.add(Dense(units=128, kernel_initializer='normal', activation='relu')) # hidden and input layers\n",
    "dnn_model.add(Dense(units=1, kernel_initializer='normal',activation='linear')) # output layer\n",
    "\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a checkpoint callback\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001, decay = 0.005)\n",
    "\n",
    "dnn_model.compile(optimizer=opt,\n",
    "              loss='mean_absolute_error',\n",
    "              metrics=['mean_absolute_error'])\n",
    "\n",
    "dnn_model_var = dnn_model.fit(\n",
    "                        X_train_scaled,\n",
    "                        y_train_scaled,\n",
    "                        epochs=50,\n",
    "                        shuffle=True,\n",
    "                        verbose=2,\n",
    "                        batch_size=32, \n",
    "                        validation_split = 0.33,\n",
    "                        callbacks=callbacks_list\n",
    "                    )\n",
    "\n",
    "plt.plot(dnn_model_var.history['loss'])\n",
    "plt.plot(dnn_model_var.history['val_loss'])\n",
    "plt.title('Deep Neural Network Model Loss (lr=0.001, decay = 0.005)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001, decay = 0.05, amsgrad = True)\n",
    "\n",
    "dnn_model_var = dnn_model.fit(\n",
    "                        X_train_scaled,\n",
    "                        y_train_scaled,\n",
    "                        epochs=50,\n",
    "                        shuffle=True,\n",
    "                        verbose=2,\n",
    "                        batch_size=32, \n",
    "                        validation_split = 0.33,\n",
    "                        callbacks=callbacks_list\n",
    "                    )\n",
    "\n",
    "plt.plot(dnn_model_var.history['loss'])\n",
    "plt.plot(dnn_model_var.history['val_loss'])\n",
    "plt.title('Deep Neural Network Model Loss (lr=0.001, decay = 0.05, amsgrad = True)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model.compile(optimizer='adam',\n",
    "              loss='mean_absolute_error',\n",
    "              metrics=['mean_absolute_error'])\n",
    "\n",
    "dnn_model_var = dnn_model.fit(\n",
    "                        X_train_scaled,\n",
    "                        y_train_scaled,\n",
    "                        epochs=50,\n",
    "                        shuffle=True,\n",
    "                        verbose=2,\n",
    "                        batch_size=32, \n",
    "                        validation_split = 0.33,\n",
    "                        callbacks=callbacks_list\n",
    "                    )\n",
    "\n",
    "plt.plot(dnn_model_var.history['loss'])\n",
    "plt.plot(dnn_model_var.history['val_loss'])\n",
    "plt.title('Deep Neural Network Model Loss - stock Adam')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model_train_loss, dnn_model_train_accuracy = dnn_model.evaluate(\n",
    "    X_train_scaled, y_train_scaled, verbose=2)\n",
    "print(\n",
    "    f\"Deep Neural Network Evaluation - Train Loss: {dnn_model_train_loss}, Train Accuracy: {dnn_model_train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load wights file of the best model :\n",
    "# wights_file = 'Weights-478--18738.19831.hdf5' # choose the best checkpoint \n",
    "# dnn_model.load_weights(wights_file) # load it\n",
    "# dnn_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predicted_dnn = dnn_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test and train - treat separately from start\n",
    "#lr (adam), relu (alpha - make it 0.01) dropout, add more layers to dnn (to the power of 2), drop date and don't shuffle\n",
    "# previous week/ rolling avg of sales (drop first row)\n",
    "# t-1 and t column\n",
    "# Try vanilla and stacked LSTMs (shampoo example) https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "#Edit encoded_df to have one-hot columns for dept(99), store(45), year(4), month(12), day(7) and run model again to check accuracy\n",
    "#Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting results using model for test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing, One-hot encoding, Features-Target Split, Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the features and store information into the test_csv\n",
    "test_merged_df = (test_csv.merge(stores_csv, how='left', on='Store')).merge(features_csv, how='left', on = ['Date', 'Store'])\n",
    "del test_merged_df['IsHoliday_y'] #delete duplicate column\n",
    "test_merged_df = test_merged_df.rename(columns={\"IsHoliday_x\": \"IsHoliday\"})\n",
    "\n",
    "#Fill empty markkdown sales with 0\n",
    "test_merged_df['MarkDown1'] = test_merged_df['MarkDown1'].fillna(0)\n",
    "test_merged_df['MarkDown2'] = test_merged_df['MarkDown2'].fillna(0)\n",
    "test_merged_df['MarkDown3'] = test_merged_df['MarkDown3'].fillna(0)\n",
    "test_merged_df['MarkDown4'] = test_merged_df['MarkDown4'].fillna(0)\n",
    "test_merged_df['MarkDown5'] = test_merged_df['MarkDown5'].fillna(0)\n",
    "\n",
    "#Convert date column to datetime data type\n",
    "test_merged_df['Date'] = pd.to_datetime(test_merged_df['Date'])\n",
    "\n",
    "#Extract year, month and day from the datestamps\n",
    "test_merged_df['Year'] = test_merged_df['Date'].dt.year\n",
    "test_merged_df['Month'] = test_merged_df['Date'].dt.month\n",
    "test_merged_df['Day'] = test_merged_df['Date'].dt.day\n",
    "\n",
    "test_merged_df.drop(['Date'], axis=1, inplace=True)\n",
    "\n",
    "#Add two columns - 4-week rolling average ('rolling_4_sales') and previous week sales('t-1_sales')\n",
    "test_merged_df['rolling_4_sales'] = test_merged_df['Weekly_Sales'].rolling(4).mean()\n",
    "test_merged_df['t-1_sales'] = test_merged_df['Weekly_Sales'].shift(periods=1, freq=None, axis=0)\n",
    "\n",
    "#Use get_dummies to one-hot encode the 'Type'  and 'IsHoliday' column - one-hot encoding this column as it has >2 unique values\n",
    "test_encoded_df = pd.get_dummies(test_merged_df, columns=[\"Type\", 'IsHoliday'])\n",
    "\n",
    "# Features-Target Split\n",
    "y_test = test_encoded_df['Weekly_Sales'].values\n",
    "y_test = y_test.reshape(-1,1)\n",
    "X_test = test_encoded_df.drop('Weekly_Sales', axis=1)\n",
    "feature_names = X_test.columns\n",
    "\n",
    "#Scaling\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model1_test_loss, nn_model1_test_accuracy = nn_model1.evaluate(\n",
    "    X_test_scaled, y_test_scaled, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network Evaluation - Model 1 - Test Loss: {nn_model1_test_loss}, Test Accuracy: {nn_model1_test_accuracy}\")\n",
    "\n",
    "nn_model2_test_loss, nn_model2_test_accuracy = nn_model2.evaluate(\n",
    "    X_test_scaled, y_test_scaled, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network Evaluation - Model 2 - Test Loss: {nn_model2_test_loss}, Test Accuracy: {nn_model2_test_accuracy}\")\n",
    "\n",
    "dnn_model1_test_loss, dnn_model1_test_accuracy = dnn_model1.evaluate(\n",
    "    X_test_scaled, y_test_scaled, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network Evaluation - Model 1 - Test Loss: {dnn_model1_test_loss}, Test Accuracy: {dnn_model1_test_accuracy}\")\n",
    "\n",
    "dnn_model2_test_loss, dnn_model2_test_accuracy = dnn_model2.evaluate(\n",
    "    X_test_scaled, y_test_scaled, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network Evaluation - Model 2 - Test Loss: {dnn_model2_test_loss}, Test Accuracy: {dnn_model2_test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_nn1 = nn_model1.predict(X_test_scaled)\n",
    "\n",
    "MAE1 = mean_absolute_error(y_test_scaled , y_predicted_nn1)\n",
    "MAE1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_nn2 = nn_model2.predict(X_test_scaled)\n",
    "\n",
    "MAE2 = mean_absolute_error(y_test_scaled , y_predicted_nn2)\n",
    "MAE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_dnn1 = dnn_model1.predict(X_test_scaled)\n",
    "\n",
    "MAE3 = mean_absolute_error(y_test_scaled , y_predicted_dnn1)\n",
    "MAE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_dnn2 = dnn_model1.predict(X_test_scaled)\n",
    "\n",
    "MAE4 = mean_absolute_error(y_test_scaled , y_predicted_dnn2)\n",
    "MAE4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ravel the arrays\n",
    "y_test_minmaxscaled = y_test_scaled.ravel()\n",
    "NN1_predictions = y_predicted_nn1.ravel()\n",
    "NN2_predictions = y_predicted_nn2.ravel()\n",
    "DNN1_predictions = y_predicted_dnn1.ravel()\n",
    "DNN2_predictions = y_predicted_dnn2.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save y_test and predictions in a dataframe\n",
    "Predictions_df = pd.DataFrame({\"Y Test\":y_test_minmaxscaled,\n",
    "                               \"NN 1 Predictions\":NN1_predictions,\n",
    "                               \"NN 2 Predictions\":NN2_predictions,\n",
    "                               \"DNN 1 Predictions\":DNN1_predictions,\n",
    "                               \"DNN 2 Predictions\":DNN2_predictions})\n",
    "\n",
    "#Export dataframe\n",
    "Predictions_df.to_csv('NN Predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
